# 聚类算法实现文档

## 概述
本文档详细描述了在 `todo.py` 文件中实现的两种聚类算法：K-means算法和谱聚类算法。所有实现均基于NumPy库，未使用任何现成的聚类算法库。

## 1. K-means算法实现

### 算法原理
K-means是一种基于距离的聚类算法，通过迭代优化将数据点分配到k个簇中，使得簇内平方和最小。

### 实现细节

#### 初始化阶段
```python
np.random.seed(42)
centers = X[np.random.choice(N, k, replace=False)]
```
- 设置随机种子确保结果可重现
- 从数据点中随机选择k个点作为初始聚类中心

#### 主循环（最大100次迭代）

**步骤1：分配数据点到最近聚类中心**
```python
for i in range(N):
    distances = np.linalg.norm(X[i] - centers, axis=1)
    idx[i] = np.argmin(distances)
```
- 计算每个点到所有聚类中心的欧氏距离
- 将点分配到距离最近的聚类中心

**步骤2：更新聚类中心**
```python
new_centers = np.zeros((k, P))
for j in range(k):
    points_in_cluster = X[idx == j]
    if len(points_in_cluster) > 0:
        new_centers[j] = np.mean(points_in_cluster, axis=0)
    else:
        new_centers[j] = centers[j]  # 保持原中心不变
```
- 计算每个簇中所有点的均值作为新的聚类中心
- 处理空簇情况：如果某个簇没有数据点，保持原中心不变

**步骤3：收敛检查**
```python
if np.allclose(centers, new_centers):
    break
centers = new_centers
```
- 使用 `np.allclose` 检查聚类中心是否收敛
- 如果中心点变化小于容差，则提前终止

### 算法复杂度
- 时间复杂度：O(N×k×iter×P)，其中N是数据点数，k是簇数，iter是迭代次数，P是特征维度
- 空间复杂度：O(N×P + k×P)

## 2. 谱聚类算法实现

### 算法原理
谱聚类基于图论和线性代数，通过构建数据的相似度图，利用图的谱特性进行聚类。

### 实现细节

#### 步骤1：构建度矩阵D
```python
D = np.diag(np.sum(W, axis=1))
```
- 度矩阵是对角矩阵，对角线元素为每个节点的度（连接权重和）
- 反映每个节点的重要性

#### 步骤2：计算拉普拉斯矩阵L
```python
L = D - W
```
- 拉普拉斯矩阵是度矩阵减去邻接矩阵
- 描述了图的局部结构特性

#### 步骤3：计算归一化拉普拉斯矩阵
```python
D_sqrt_inv = np.diag(1.0 / np.sqrt(np.sum(W, axis=1)))
L_sym = D_sqrt_inv @ L @ D_sqrt_inv
```
- 计算度矩阵的逆平方根
- 构建对称归一化拉普拉斯矩阵：L_sym = D^(-1/2) * L * D^(-1/2)
- 这种归一化可以处理不同密度的簇

#### 步骤4：特征值分解
```python
eigenvalues, eigenvectors = np.linalg.eigh(L_sym)
```
- 使用 `np.linalg.eigh` 计算对称矩阵的特征值和特征向量
- 特征值按升序排列

#### 步骤5：选择特征向量
```python
X = eigenvectors[:, :k]
```
- 选择前k个最小特征值对应的特征向量
- 这些特征向量包含了图的聚类信息

#### 步骤6：特征向量归一化
```python
X_normalized = X / np.linalg.norm(X, axis=1, keepdims=True)
```
- 对特征向量进行行归一化
- 确保所有数据点在单位球面上

#### 步骤7：最终聚类
```python
X_normalized = X_normalized.astype(float)
idx = kmeans(X_normalized, k)
```
- 将归一化后的特征向量作为新的特征空间
- 使用之前实现的k-means算法进行最终聚类

### 算法复杂度
- 时间复杂度：O(N³ + N×k×iter×k)，其中N³是特征分解的复杂度
- 空间复杂度：O(N² + N×k)

## 3. 辅助函数：KNN图构建

### 函数功能
基于K近邻构建邻接矩阵W，用于谱聚类。

### 实现细节
```python
def knn_graph(X, k, threshold):
    N = X.shape[0]
    W = np.zeros((N, N))
    aj = cdist(X, X, 'euclidean')
    for i in range(N):
        index = np.argsort(aj[i])[:(k+1)]
        W[i, index] = 1
        W[i, i] = 0  # 移除自连接
    W[aj >= threshold] = 0  # 应用距离阈值
    return W
```

#### 关键步骤
1. **距离计算**：使用 `cdist` 计算所有点对之间的欧氏距离
2. **K近邻选择**：对每个点选择k+1个最近邻（包括自身）
3. **邻接矩阵构建**：将K近邻关系转换为0-1邻接矩阵
4. **自连接移除**：设置对角线为0，避免自连接
5. **阈值过滤**：移除距离超过阈值的连接

## 4. 数值稳定性考虑

### K-means算法
- 随机种子设置确保结果可重现
- 空簇处理避免除以零错误
- 收敛容差设置防止无限循环

### 谱聚类算法
- 使用 `np.linalg.eigh` 处理对称矩阵，确保特征值为实数
- 度矩阵逆平方根计算时避免除零（假设图是连通的）
- 特征向量归一化确保数值稳定性

## 5. 参数选择建议

### K-means参数
- `max_iters = 100`：对于大多数数据集足够收敛
- 随机种子：固定为42确保实验可重复

### 谱聚类参数
- 特征向量数量k：应等于期望的簇数
- KNN参数：需要根据实际情况调整k和threshold

## 6. 算法优缺点

### K-means
**优点**：
- 简单高效，适合大规模数据
- 收敛速度快
- 结果易于解释

**缺点**：
- 假设簇为球形，对非凸形状效果差
- 对初始中心敏感
- 对异常值敏感

### 谱聚类
**优点**：
- 可以处理非凸形状的簇
- 对数据分布假设较少
- 能够识别复杂的聚类结构

**缺点**：
- 计算复杂度高，不适合大规模数据
- 需要构建相似度图，参数选择敏感
- 特征分解计算成本高

## 7. 使用示例

```python
import numpy as np
from todo import kmeans, spectral, knn_graph

# 生成测试数据
X = np.random.randn(100, 2)

# K-means聚类
k = 3
labels_kmeans = kmeans(X, k)

# 谱聚类
W = knn_graph(X, k=10, threshold=2.0)
labels_spectral = spectral(W, k)
```

这个实现提供了完整的聚类功能，可以根据具体需求进行调整和优化。