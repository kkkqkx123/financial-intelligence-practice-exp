# 谱聚类第一轮调优结果分析

## 数据特征分析
通过分析cluster_data.mat数据，发现这是一个典型的**内外圆数据集**：
- 数据形状：(4078, 2) - 4078个二维数据点
- 数据范围：X轴[-1.00, 0.99]，Y轴[-0.99, 1.00]
- 距离分布：最小距离0.020，最大距离1.000，平均距离0.674

## 问题诊断
**谱聚类在内外圆数据上的根本问题**：
1. **几何结构不匹配**：谱聚类基于图论，假设数据具有**流形结构**，但内外圆是**同心圆结构**
2. **KNN图构建缺陷**：当前knn_graph函数使用欧氏距离阈值，无法有效分离内外圆
3. **特征向量选择问题**：对于同心圆数据，第二小特征值对应的特征向量可能无法区分内外圆

## 搜索的参数
- **k近邻数 (k)**：8, 12, 16（3个值）
- **距离阈值 (threshold)**：0.8, 1.4, 2.0（3个值）
- **总搜索次数**：9次（3个k值 × 3个阈值）→ 限制为最多8次

## 第一轮调优结果
经测试，最佳参数为：
- **k近邻数 (k)**：12
- **距离阈值 (threshold)**：0.8/1.4/2.0
- **评分 (Score)**：0.8800

**但效果仍不理想**：谱聚类无法有效区分内外两个簇，因为基于欧氏距离的KNN图无法捕捉同心圆的几何特性。

## 改进方向
1. **改进KNN图构建**：使用基于角度或径向距离的相似度度量
2. **自适应阈值**：根据数据密度动态调整阈值
3. **多尺度谱聚类**：结合不同尺度的相似度信息
4. **特征工程**：在谱聚类前进行适当的特征变换

结果文件：
- 分类聚类\clustering_results\spectral_tuning_results-1.csv
- 分类聚类\clustering_results\spectral_results-1.csv
- 分类聚类\clustering_results\spectral_best_params-1.csv
- 分类聚类\clustering_results\Clustering_Spectral_k=12,_threshold=0.40.png

# 谱聚类第二轮调优结果分析

搜索的参数：
- **k**：8, 12, 16（3个值）
- **距离阈值 (threshold)**：0.2, 0.4, 0.6（3个值）【在上一次的0.8的基础上向下搜索】
- **总搜索次数**：9次（3个k值 × 3个阈值）→ 限制为最多8次

经测试，最佳参数为：
- **k**：12
- **距离阈值 (threshold)**：0.2, 0.4
- **评分 (Score)**：0.8800

结果见：
分类聚类\clustering_results\Clustering_Spectral_k=12,_threshold=0.40.png

根据图片可见，聚类效果不理想，没用区分内外2个簇。考虑大幅增大k。


# 最终调优结果分析
最终调优参数为：
- **k**：200
- **距离阈值 (threshold)**：0.20
- **评分 (Score)**：0.8146

结果见：
分类聚类\clustering_results\Clustering_Spectral_k=200,_threshold=0.20.png

根据图片可见，聚类效果良好，内外圆正确划分。
你的观察非常关键——**将 `k` 增大到 200 后能正确区分内外圆，很可能是因为这里的 `k` 不是聚类数（`n_clusters`），而是 kNN 亲和矩阵中的邻居数（`n_neighbors`）**。这与你最初提供的图中标题 “k=6” 的含义一致（在谱聚类的上下文中，`k` 常指 `n_neighbors`，而非聚类数）。下面我来详细解释原因，并说明“离散点很多”为何是关键因素。

---

### 一、为什么 `k`（`n_neighbors`）增大到 200 能改善结果？
#### 📌 核心原因：**`n_neighbors` 需适配数据密度和结构**
在谱聚类中，当使用 `affinity='knn'` 时，`n_neighbors`（即你提到的 `k`）决定了每个点连接多少个最近邻居，从而构建亲和矩阵（相似度矩阵）。  
- **同心圆数据的特点**：  
  - 内圈半径小 → 点密集（点之间距离小）。  
  - 外圈半径大 → 点相对稀疏（点之间距离大）。  
  - 两圈之间存在**间隙**（gap），理想情况下圈内点只与同圈点连接，圈间无连接。  

- **`n_neighbors` 过小的问题（如 `k=6`）**：  
  - 在外圈稀疏区域，点到内圈点的距离可能小于外圈其他点的距离（因为内圈半径小，内圈点可能更“近”）。  
  - 例如：外圈一个点，其 6 个最近邻居中可能包含内圈点 → 亲和矩阵中出现**圈间虚假连接** → 谱聚类无法区分内外圈（如你提供的图中红蓝混杂）。  
  - **本质**：`n_neighbors` 太小，无法保证外圈自身的连通性，导致 kNN 图被“撕裂”或跨圈连接。

- **`n_neighbors` 增大到 200 的改善机制**：  
  - 当数据点足够多时（如总点数 > 10,000），内圈和外圈各自包含大量点（例如内圈 3,000 点、外圈 7,000 点）。  
  - `n_neighbors=200` 满足：  
    - **内圈**：200 个最近邻居全部来自内圈（因为内圈点密集，200 < 内圈点数）。  
    - **外圈**：200 个最近邻居全部来自外圈（因为外圈虽稀疏，但总点数多，200 < 外圈点数）。  
  - **结果**：kNN 图形成两个**完全分离的连通组件**（内圈一个、外圈一个），谱聚类能精准识别这两个簇。  
  - **关键**：`n_neighbors` 必须 **< min(内圈点数, 外圈点数)**，否则内圈点可能被迫连接外圈点（导致失败）。200 这个值恰好落在“安全区间”内。

> ✅ **简单总结**：  
> - `k=6` 太小 → 外圈点“误连”内圈 → 混杂。  
> - `k=200` 适中 → 圈内连通、圈间无连接 → 正确分离。  
> - **这不是因为 `k` 增大本身好，而是因为 `k` 匹配了数据密度**。

---

### 二、“离散点很多”是必要前提
你提到“是因为离散点很多吗？”——**是的，点数足够大是成功的关键**。原因如下：
1. **`n_neighbors` 的有效性依赖于数据规模**：  
   - 如果总点数少（如 100 点），内圈可能只有 50 点。此时 `n_neighbors=200` 会**强制连接圈间点**（因为 200 > 50），导致更差的结果。  
   - 但你的数据点很多（例如 > 5,000 点），内圈和外圈各自点数远大于 200 → `n_neighbors=200` 能安全地“隔离”两个环。

2. **验证点数是否足够的方法**：  
   - 检查内圈/外圈的最小点数：  
     ```python
     # 假设 X 是数据，y 是真实标签（0=内圈,1=外圈）
     n_inner = sum(y == 0)
     n_outer = sum(y == 1)
     print(f"内圈点数: {n_inner}, 外圈点数: {n_outer}")
     ```
   - **成功条件**：`n_neighbors < min(n_inner, n_outer)`。  
     例如：若 `n_inner=2500`，则 `n_neighbors=200` 安全；若 `n_inner=150`，则 `n_neighbors=200` 会失败。

3. **为什么点少时增大 `k` 会失败？**  
   - 点少时，增大 `n_neighbors` 会引入圈间连接（因为点不够“填满”邻居列表）。  
   - 例如：内圈只有 100 点，`n_neighbors=200` 时，内圈点的邻居会包含外圈点 → 亲和矩阵出现“桥接” → 谱聚类将两圈合并。

> 💡 **结论**：  
> **增大 `k`（`n_neighbors`）能正确区分内外圆的前提是：数据点足够多，且 `n_neighbors` 小于最小圈的点数。**  
> 你的场景中“离散点很多”是必要条件，否则此操作反而有害。

---

### 三、如何科学调优 `n_neighbors`？
既然你已发现 `n_neighbors=200` 有效，但需要避免盲目调参，建议以下方法：

#### ✅ 步骤 1：确定数据点规模
- 计算内圈/外圈的最小点数 `n_min = min(n_inner, n_outer)`。
- **安全范围**：`n_neighbors` 应选在 `[5, n_min/2]` 区间内（避免过小导致不连通，或过大导致跨圈）。

#### ✅ 步骤 2：用“连通性”验证 `n_neighbors`
- 目标：确保 kNN 图中**每个真实簇是连通的**，且**簇间无连接**。  
- 操作：  
  ```python
  from sklearn.neighbors import kneighbors_graph
  import numpy as np

  # 生成 kNN 图 (n_neighbors 是你要测试的值)
  adj_matrix = kneighbors_graph(X, n_neighbors=200, mode='connectivity', include_self=False)
  # 检查连通组件数量
  from scipy.sparse.csgraph import connected_components
  n_components, labels = connected_components(adj_matrix, directed=False)
  print(f"连通组件数: {n_components}")
  ```
  - **理想结果**：`n_components=2`（内圈+外圈各一个组件）。  
  - 若 `n_components > 2`：`n_neighbors` 太小，圈内不连通。  
  - 若 `n_components < 2`：`n_neighbors` 太大，圈间连接了。

#### ✅ 步骤 3：网格搜索 `n_neighbors`
- 在 `[5, n_min/2]` 范围内搜索，选择使轮廓系数最高的值：  
  ```python
  from sklearn.metrics import silhouette_score

  best_score = -1
  for k in range(5, int(n_min/2), 5):  # 步长5
      labels = SpectralClustering(
          n_clusters=2, 
          affinity='knn',
          n_neighbors=k,
          assign_labels='discretize'
      ).fit_predict(X)
      score = silhouette_score(X, labels)
      if score > best_score:
          best_k = k
          best_score = score
  print(f"最佳 n_neighbors: {best_k}")
  ```
