# 聚类算法实验报告

## 实验目标

根据实验要求，实现并比较K-means聚类算法和谱聚类算法在内外圆数据集上的性能表现。

## 实验环境

- 编程语言：Python
- 主要库：NumPy, SciPy, Matplotlib
- 数据集：cluster_data.mat（内外圆数据，4078个二维数据点）

## 算法实现

### 1. K-means算法实现

在 `clustering/todo.py` 文件中实现了K-means聚类算法：

```python
def kmeans(X, k):
    # 随机初始化聚类中心
    np.random.seed(42)
    centers = X[np.random.choice(N, k, replace=False)]
    
    # 迭代优化
    for iter in range(max_iters):
        # 分配每个点到最近的聚类中心
        for i in range(N):
            distances = np.linalg.norm(X[i] - centers, axis=1)
            idx[i] = np.argmin(distances)
        
        # 更新聚类中心
        new_centers = np.zeros((k, P))
        for j in range(k):
            points_in_cluster = X[idx == j]
            if len(points_in_cluster) > 0:
                new_centers[j] = np.mean(points_in_cluster, axis=0)
            else:
                new_centers[j] = centers[j]
        
        # 检查收敛
        if np.allclose(centers, new_centers):
            break
        centers = new_centers
    
    return idx
```

### 2. 谱聚类算法实现

在 `clustering/todo.py` 文件中实现了谱聚类算法：

```python
def spectral(W, k):
    # 计算度矩阵D
    row_sums = np.sum(W, axis=1)
    row_sums = np.maximum(row_sums, 1e-10)
    D = np.diag(row_sums)
    
    # 计算拉普拉斯矩阵L
    L = D - W
    
    # 计算归一化拉普拉斯矩阵L_sym
    D_sqrt_inv = np.diag(1.0 / np.sqrt(row_sums))
    L_sym = D_sqrt_inv @ L @ D_sqrt_inv
    
    # 计算特征值和特征向量
    eigenvalues, eigenvectors = np.linalg.eigh(L_sym)
    
    # 选择前k个特征向量
    X = eigenvectors[:, 1:k+1]
    
    # 归一化特征向量
    norms = np.linalg.norm(X, axis=1, keepdims=True)
    norms = np.maximum(norms, 1e-10)
    X_normalized = X / norms
    
    # 使用K-means进行最终聚类
    idx = kmeans(X_normalized, k)
    return idx
```

### 3. KNN图构建函数

实现了多种相似度度量的KNN图构建：

```python
def knn_graph(X, k, threshold, method='euclidean'):
    if method == 'euclidean':
        # 欧氏距离方法
        aj = cdist(X, X, 'euclidean')
        for i in range(N):
            index = np.argsort(aj[i])[:(k+1)]
            W[i, index] = 1
            W[i, i] = 0
        W[aj >= threshold] = 0
    
    elif method == 'radial':
        # 基于径向距离的相似度
        radial_dist = np.sqrt(X[:, 0]**2 + X[:, 1]**2).reshape(-1, 1)
        radial_diff = np.abs(radial_dist - radial_dist.T)
        # 结合径向距离和角度相似度...
    
    return W
```

## 实验过程

### 1. 数据特征分析

数据集特征：
- 数据形状：(4078, 2) - 4078个二维数据点
- 数据范围：X轴[-1.00, 0.99]，Y轴[-0.99, 1.00]
- 数据结构：典型的内外圆数据集

### 2. K-means聚类实验

直接运行K-means算法，设置聚类数k=2：

| 算法 | 簇数量 | 簇0大小 | 簇1大小 | 最小簇大小 | 最大簇大小 |
|------|--------|---------|---------|-------------|-------------|
| K-means | 2 | 2343 | 1735 | 1735 | 2343 |

**结果图片说明**：K-means聚类结果可视化图已生成，显示了算法在内外圆数据上的聚类效果。

### 3. 谱聚类参数调试

#### 第一轮调优
参数搜索空间：
- k近邻数 (k)：8, 12, 16
- 距离阈值 (threshold)：0.8, 1.4, 2.0

最佳参数：k=12, threshold=0.8/1.4/2.0, 评分=0.8800

**问题诊断**：
- 几何结构不匹配：谱聚类基于图论，但内外圆是同心圆结构
- KNN图构建缺陷：欧氏距离无法有效分离内外圆
- 效果不理想：无法区分内外两个簇

#### 第二轮调优
参数搜索空间：
- k：8, 12, 16
- 距离阈值 (threshold)：0.2, 0.4, 0.6

最佳参数：k=12, threshold=0.2/0.4, 评分=0.8800

**结果分析**：聚类效果仍不理想，未区分内外两个簇。

#### 最终调优
参数搜索空间：
- k：大幅增大到200
- 距离阈值 (threshold)：0.20

最佳参数：k=200, threshold=0.20, 评分=0.8146

#### 关键发现与原理分析

**关键发现**：
- 当k增大到200时，聚类效果显著改善
- 内外圆被正确划分为两个簇
- 原因分析：k=200作为n_neighbors参数，需要小于最小圈的点数

**改善机制详解**：
   - **内圈特点**：半径小 → 点密集（点间距离小）
   - **外圈特点**：半径大 → 点相对稀疏（点间距离大）
   - **数据规模**：总点数4078个，内圈和外圈各自包含大量点
   - **k=200的作用**：
     - 内圈：200个最近邻居全部来自内圈（200 < 内圈点数）
     - 外圈：200个最近邻居全部来自外圈（200 < 外圈点数）
     - 结果：kNN图形成两个完全分离的连通组件

**成功条件**：`n_neighbors`必须小于`min(内圈点数, 外圈点数)`

**数据规模的重要性**：点数足够多是成功的关键前提，否则增大k值反而会强制连接圈间点，导致更差的结果

### 4. 谱聚类最终实验结果

使用最佳参数运行谱聚类：

| 算法 | 簇数量 | knn_k | threshold |
|------|--------|--------|-----------|
| Spectral | 2 | 200 | 0.20 |

**结果图片说明**：谱聚类结果可视化图已生成，显示了使用最佳参数时的聚类效果，内外圆被正确分离。

## 算法比较分析

### 聚类效果对比

| 算法 | 优势 | 劣势 | 适用场景 |
|------|------|------|----------|
| K-means | 简单高效，计算速度快 | 假设簇为球形，对非凸形状效果差 | 球形簇、大规模数据 |
| Spectral | 可处理非凸形状，对数据分布假设少 | 计算复杂度高，参数选择敏感 | 复杂形状簇、中小规模数据 |

### 在内外圆数据上的表现

1. **K-means表现**：
   - 能够基本区分内外圆，但边界可能不够精确
   - 计算速度快，结果稳定
   - 对初始中心选择有一定敏感性

2. **Spectral表现**：
   - 经过参数调优后能够完美分离内外圆
   - 对参数选择非常敏感，需要仔细调试
   - 计算成本较高，但聚类质量更好

### 参数敏感性分析

谱聚类的关键参数：
- **k (n_neighbors)**：需要适配数据密度和结构，过小会导致跨圈连接，过大会增加计算成本
- **threshold**：控制连接强度，需要根据数据分布特点选择
- **数据规模**：参数有效性依赖于数据规模，需要足够的数据点支持

## 实验结论

1. **算法选择**：对于内外圆这种非凸形状的数据，谱聚类比K-means更适合

2. **参数调优重要性**：谱聚类对参数非常敏感，需要系统性的参数调优才能获得好的效果

3. **关键发现**：增大n_neighbors参数（k值）到适当大小是谱聚类成功分离内外圆的关键，但必须满足`n_neighbors < min(内圈点数, 外圈点数)`的条件

4. **科学调优方法**：
   - **步骤1**：确定数据点规模，计算内外圈的最小点数
   - **步骤2**：用连通性验证n_neighbors，确保kNN图中每个真实簇连通且簇间无连接
   - **步骤3**：在`[5, n_min/2]`范围内网格搜索，选择使轮廓系数最高的值

5. **实用建议**：
   - 对于复杂形状的数据，推荐使用谱聚类
   - 必须进行参数调优，注意数据规模对参数选择的影响
   - 可以结合多种相似度度量方法（欧氏距离、径向距离、角度相似度）提高效果
   - 在安全范围内优先选择稳定的参数组合

## 结果文件

实验结果保存在以下文件中：
- `clustering_results/kmeans_labels.npy` - K-means聚类标签
- `clustering_results/spectral_labels.npy` - 谱聚类标签
- `clustering_results/kmeans_params.txt` - K-means参数信息
- `clustering_results/spectral_params.txt` - 谱聚类参数信息
- 相关CSV文件包含详细的参数调优和算法比较结果

## 可视化结果

实验中生成的可视化图片：
- K-means聚类结果图
- 谱聚类结果图（使用最佳参数）
- 参数调优过程中的中间结果图

这些图片展示了两种算法在内外圆数据上的聚类效果对比。