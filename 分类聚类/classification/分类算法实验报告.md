# 分类算法实验报告

## 实验目标

根据实验要求，实现并比较支持向量机（SVM）、逻辑回归（Logistic Regression）和感知器（Perceptron）三种分类算法在二维线性可分数据集上的性能表现。

## 实验环境

- 编程语言：Python
- 主要库：NumPy, SciPy, Matplotlib
- 数据集：生成的二维线性可分数据（100个样本点）
- 训练集/测试集划分：70%训练，30%测试

## 算法实现

### 1. 支持向量机（SVM）实现

在 `classification/todo.py` 文件中实现了SVM分类器：

```python
def func(X, y):
    # 向X添加偏置项
    X_bias = np.vstack((np.ones((1, N)), X))
    
    # 定义SVM的目标函数（合页损失 + L2正则化）
    def objective(w):
        w = w.reshape(-1, 1)
        margins = y * (w.T @ X_bias).flatten()
        hinge_loss = np.maximum(0, 1 - margins)
        return 0.5 * np.sum(w[1:]**2) + np.sum(hinge_loss)
    
    # 使用BFGS优化算法求解
    result = minimize(objective, w_init, method='BFGS')
    return result.x.reshape(-1, 1)
```

### 2. 逻辑回归实现

实现了逻辑回归分类器：

```python
def logistic_regression(X, y):
    # 定义逻辑回归的目标函数（对数损失）
    def objective(w):
        w = w.reshape(-1, 1)
        z = w.T @ X_bias
        predictions = 1 / (1 + np.exp(-z.flatten()))
        # 计算交叉熵损失
        loss = -np.mean(y_binary * np.log(predictions) + (1 - y_binary) * np.log(1 - predictions))
        return loss
    
    # 使用BFGS优化算法求解
    result = minimize(objective, w_init, method='BFGS')
    return result.x.reshape(-1, 1)
```

### 3. 感知器实现

实现了感知器算法：

```python
def perceptron(X, y):
    # 初始化权重为零
    w = np.zeros((P + 1, 1))
    
    # 感知器学习算法
    for iteration in range(max_iterations):
        converged = True
        for i in range(N):
            prediction = np.sign(w.T @ X_bias[:, i])
            if prediction != y[i]:
                # 权重更新规则
                w += learning_rate * y[i] * X_bias[:, i:i+1]
                converged = False
        
        if converged:
            break  # 算法收敛
    
    return w
```

## 实验过程

### 1. 数据生成

使用 `gen_data.py` 生成二维线性可分数据集：
- 数据范围：[-1, 1] × [-1, 1]
- 样本数量：100个数据点
- 标签生成：基于随机生成的线性决策边界
- 确保数据线性可分且包含正负两类样本

### 2. 实验设置

- **训练集大小**：70个样本（70%）
- **测试集大小**：30个样本（30%）
- **重复实验**：1000次独立实验
- **性能指标**：分类错误率（错误样本数/总样本数）

### 3. 错误率计算

实现了统一的错误率计算函数：

```python
def compute_error(X, y, w):
    X_bias = np.vstack((np.ones((1, X.shape[1])), X))
    predictions = np.sign(w.T @ X_bias)
    predictions[predictions == 0] = 1  # 处理预测值为0的情况
    errors = np.sum(predictions.flatten() != y.flatten())
    return errors / y.shape[1]
```

## 实验结果

### 算法性能对比

| 算法 | 训练错误率 | 测试错误率 | 标准差 |
|------|------------|------------|--------|
| 支持向量机（SVM） | 0.0308 | 0.0433 | 较低 |
| 逻辑回归 | 0.0000 | 0.0186 | 中等 |
| 感知器 | 0.0000 | 0.0191 | 中等 |

### 结果分析

1. **训练表现**：
   - 逻辑回归和感知器在训练集上达到0错误率，表明这两种算法能够完美拟合训练数据
   - SVM在训练集上有约3%的错误率，这可能是由于正则化项的存在，SVM倾向于找到最大间隔的决策边界而非完美拟合所有训练点

2. **泛化能力**：
   - 逻辑回归表现最佳，测试错误率仅为1.86%
   - 感知器与逻辑回归性能相近，测试错误率为1.91%
   - SVM的测试错误率为4.33%，相对较高

3. **算法特性分析**：
   - **逻辑回归**：通过概率建模，对线性可分数据表现优异，具有良好的泛化能力
   - **感知器**：作为最简单的线性分类器，在本实验中表现良好，但算法稳定性相对较低
   - **SVM**：虽然训练错误率较高，但具有更好的理论保证和鲁棒性

## 可视化结果

实验生成了SVM分类结果的可视化图片，展示了：
- 红色圆点：正类样本
- 绿色圆点：负类样本  
- 实线：真实的决策边界
- 虚线：学习得到的决策边界

【插入图片：SVM分类结果可视化图】

## 实验结论

1. **算法选择建议**：
   - 对于线性可分数据，逻辑回归是最佳选择，具有最低的错误率和良好的概率解释
   - 感知器算法简单有效，适合作为基线算法
   - SVM虽然在本实验中表现不是最优，但其最大间隔原理提供了更好的理论保证

2. **训练策略**：
   - 70%/30%的训练测试划分比例在实践中表现良好
   - 多次实验平均可以有效降低随机性的影响

3. **实现要点**：
   - 所有算法都正确处理了偏置项
   - 优化算法的选择对最终性能有重要影响
   - 适当的正则化可以防止过拟合

## 结果文件

实验结果保存在以下文件中：
- `classification/classification_results/classification_comparison.txt` - 算法比较结果
- `classification/SVM分类结果.png` - 可视化结果图
